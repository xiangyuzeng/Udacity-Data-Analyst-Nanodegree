{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4 Deep Models for Text and Sequences\n",
    "\n",
    "Example problem: Classifying documents into different categories: Politics, Business, Medicine.\n",
    "\n",
    "Difficulty of text problem:\n",
    "- The words that you see rarely often matter most. 'Retinopathy' appears with frequency 0.0001% in English so it may not be in your training set, but if it's found in a document that doc is likely a medical doc.\n",
    "- Often use different words to mean similar things, e.g. 'cat' and 'kitty'. Would like to share parameters between these words, so we have to learn that they are related.\n",
    "\n",
    "-> Requires collecting a lot of label data. Too much.\n",
    "\n",
    "**Solution: Unsupervised learning**: Training without any labels. \n",
    "\n",
    "**Idea: Similar words tend to occur in similar contexts.**\n",
    "\n",
    "Try to predict word's context -> Treat 'cat' and 'kitty' similarly and bring them closer together.\n",
    "\n",
    "Advantage: Don't have to worry about what the words themselves mean, only the context they appear in.\n",
    "\n",
    "Map words to small vectors called embeddings which are going to be close to each other when words have similar meanings and far apart if they don't have similar meanings.\n",
    "\n",
    "Have word representation where all cat-like things are all represented by vectors that are very similar.\n",
    "\n",
    "Model can then generalise from this pattern of cat-like things instead of learning new things for every way there is to talk about a cat.\n",
    "\n",
    "### Word2vec\n",
    "\n",
    "A way of learning these embedddings.\n",
    "\n",
    "Suppose you have a corpus of text with one sentence.\n",
    "\n",
    "For each word in this sentence, we will map it to an embedding - initially a random one.\n",
    "\n",
    "Then we will use the embedding to try to predict the context of the word. \n",
    "\n",
    "The context here is simply the words around the chosen word.Pick a random word in a window around the original word and that's your target.\n",
    "\n",
    "Train your model as though it were a supervised model. Use logistic regression. (Not deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing how embeddings are clustering together\n",
    "\n",
    "1. Nearest neighbour lookup (of the words that are closest to any given word)\n",
    "2. Try to reduce dimensionality of embedding space to 2D and plot 2D representation\n",
    "    * Naive way such as PCA loses lots of information. Need a way of projecting embeddings that preserves information, e.g. **t-SNE**.\n",
    "    \n",
    "### Comparing Embeddings\n",
    "\n",
    "1. It is best to measure closeness using **cosine distance** instead of L2 distance. We may also wish to normalise vectors to have unit norm.\n",
    "* Because length of embedding vector is not relevant to the classification.\n",
    "\n",
    "2. **Sampled Softmax**: Take only a random sample of words that are not the target (negative targets) and act as though the other words were not there. Makes things faster (more efficient) with no cost to performance. This is important because there may be many words in our vocabulary.\n",
    "\n",
    "### Words as Vectors\n",
    "\n",
    "e.g. PUPPY - DOG + CAT = KITTEN\n",
    "TALLER - TALL + SHORT = SHORTER\n",
    "\n",
    "Emergent properties of embedding vectors: let you express semantic and syntactic analogies in terms of mathematical operations.\n",
    "\n",
    "**Semantic analogy**\n",
    "$$V_{puppy} - V_{dog} \\sim  V_{kitten} - V_{cat}$$\n",
    "\n",
    "\n",
    "**Syntactic analogy**\n",
    "$$V_{taller} - V_{tall} \\sim  V_{shorter} - V_{short}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text as a Sequence of Words\n",
    "\n",
    "So far our models have only looked at inputs with fixed size, i.e. you can turn it into a vector and fit it into your neural network. When you have **sequences of varying length**, you can no longer do that.\n",
    "\n",
    "### Recurrent Networks RNNs\n",
    "\n",
    "Sequence of events. At each point in time you want to decide what's happened so far in the sequence. If your sequence is reasonably stationary, you can use the same classifier at each step. This simplifies things a lot already. \n",
    "\n",
    "But you may want to take into account the past (since this is a sequenc). You can use the state of the previous classifier as a summary of what happened before.\n",
    "\n",
    "### Backprop through time: Computing parameter updates of RNNs\n",
    "\n",
    "Calculate for as many steps as we can afford.\n",
    "\n",
    "All derivatives apply to same parameters -> Lots of correlated updates at once for the same weights. \n",
    "-> Bad for SGD, which prefers to have uncorrelated updates to its parameters to keep training stable.\n",
    "\n",
    "This makes maths unstable: either the gradients grow exponentially and go to infinity (**exploding gradient**) or go to zero and you don't end up training anything (**vanishing gradient**).\n",
    "\n",
    "#### Exploding Gradient: Gradient Clipping\n",
    "Compute the norm of the gradients and shrink the steps when the norm gets too big. (Hacky solution)\n",
    "\n",
    "#### Tackling Vanishing Gradients\n",
    "\n",
    "**Effect: Memory Loss** Vanishing Gradients make your network only remember recent events.\n",
    "\n",
    "**LSTM (Long short-term memory)**. \n",
    "Conceptually a RNN is composed of many units, with each unit being a simply neural net (set of layers). \n",
    "\n",
    "With LSTMs, we replace each module with an LSTM 'cell' and leave the general architecture unchanged.\n",
    "\n",
    "Three ops\n",
    "* Write data into memory\n",
    "* Read data from memory\n",
    "* Erase data from memory\n",
    "\n",
    "(Diagram)\n",
    "Binary instruction gates\n",
    "\n",
    "Connect to NNs: Imagine if you had continuous gates instead of binary ones.\n",
    "-> If function is continuous and differentiable, can take derivatives -> can backprop.\n",
    "\n",
    "This is LSTM.\n",
    "\n",
    "The gating value for each gate are controlled by a tiny logistic regression on the input parameters. Each of them have its own set of shared parameters.\n",
    "\n",
    "(insert diagram)\n",
    "tanh to keep output between -1.0 and 1.0.\n",
    "\n",
    "Why do they work?\n",
    "* The little gates help the NN keep the memory longer when it needs to and ignore things when it should. -> Optimisation becomes much easier and gradient vanishing vanishes. (Bit of a black box)\n",
    "\n",
    "#### LSTM Regularisation\n",
    "* L2 always works.\n",
    "* Dropout works as long as you use it on the input and output, not on the recurrent connections (connections to past and future).\n",
    "\n",
    "\n",
    "e.g. you have a model that predicts the next step of a sequence. \n",
    "\n",
    "You can use that to generate sequences.\n",
    "\n",
    "Take sequence at time t, Predict -> Sample from distribution -> feed sample to next step and predict -> Sample -> ...\n",
    "\n",
    "Alternative: Sample many times at each step. (Just sampling next prediction every time is greedy.)\n",
    "\n",
    "-> Have multiple sequences (hypotheses) that you can continue predicting from at every step. Can choose best by looking at total probability over multiple timesteps at a time. Doing this can avoid your network accidentally  making one bad choice and being stuck with that choice forever.\n",
    "-> But then number of hypotheses grows exponentially if you do this naively.\n",
    "-> Instead, use beam search: only keep most likely sequences at every time step and **prune** the rest. Works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "\n",
    "Can play legos with deep models: put them together and then use backprop to optimise the combination of models.\n",
    "\n",
    "Maps variable-length sequences to fixed-length vectors.\n",
    "\n",
    "(Can also be made to map fixed-length vectors to variable-length sequences.)\n",
    "\n",
    "Can stitch these together to map sequences or arbitrary length to other sequences of arbitrary length.\n",
    "-> e.g. machine translation.\n",
    "-> e.g. speech recognition\n",
    "\n",
    "Covnet x RNN -> Image captions e.g. http://mscoco.org\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

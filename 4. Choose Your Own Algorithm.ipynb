{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Choose Your Own Algorithm\n",
    "\n",
    "Choices:\n",
    "1. k nearest neighbours\n",
    "2. adaboost\n",
    "3. random forest\n",
    "\n",
    "2 and 3 are ensemble methods, i.e. meta classifiers built from (usually) decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "1. Fit a weak learner (e.g. a short decision tree) to the original data.\n",
    "2. Fit additional copies of the classifier on the same dataset where the weights of datapoints classified incorrectly by the previous classifire are increased.\n",
    "3. Combine the predictions from all the classifiers through a weighted majority voto or sum to produce the final prediction.\n",
    "\n",
    "Principle: fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

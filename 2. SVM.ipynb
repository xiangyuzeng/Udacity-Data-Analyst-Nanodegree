{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM: Support Vector Machines\n",
    "\n",
    "Young algorithm.\n",
    "\n",
    "Initial defn: SVM **finds (outputs) a separating line (hyperplane) between data of two classes.**\n",
    "\n",
    "Q: What makes a good separating line?\n",
    "\n",
    "A: Maximises **margin**: distance between the line and the nearest point of either of two classes.\n",
    "\n",
    "Underlying concept is to **maximise robustness**.\n",
    "\n",
    "Question diagram.\n",
    "\n",
    "**SVM prioritises correct classification over maximising margin.**\n",
    "\n",
    "BUT **outliers**: may ignore individual outliers to do the best it can in constructing a decision surface. **SVM is somewhat robust to outliers.** SOmehow mediates attempt to find maximum marginal separator and ability to ignore outliers. There is a tradeoff: can determine via parameters how willing it is to ignore outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [sk-learn documentation](http://scikit-learn.org/stable/modules/svm.html):\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "* If the number of features is much greater than the number of samples, the method is likely to give poor performances.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  \n",
    "\"\"\"\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\"\"\"\n",
    "X_test = [[2., 2.], [3., 5.]]\n",
    "predictions = clf.predict(X_test)\n",
    "y_test_true = [0, 1]\n",
    "print(\"Accuracy score: \", metrics.accuracy_score(y_test_true, predictions))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise\n",
    "import sys\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "\n",
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# We specify a kernel\n",
    "clf = SVC(kernel=\"linear\")\n",
    "\n",
    "\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "def submitAccuracy():\n",
    "    return acc\n",
    "# 0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVMs\n",
    "\n",
    "(Insert diagram)\n",
    "\n",
    "SVM is built on giving linear separation.\n",
    "\n",
    "* Previously assume inputs x,y - SVM -> Label.\n",
    "* Now have x, y, $x^2+y^2$ - SVM -> Label.\n",
    "\n",
    "SVM makes nonlinear decision surfaces by **making new features**. In the case above, $z = x^2 + y^2$ is a new feature.\n",
    "\n",
    "Now they are linearly separable?\n",
    "\n",
    "### Finding New Features using the Kernel Trick\n",
    "\n",
    "Gist: \n",
    "* Changing input space X,Y into a much larger input space $X_i$ using the kernel trick, \n",
    "* separate using SVM. \n",
    "* Take solution back to original space to get non-linear separation.\n",
    "\n",
    "**One of most central tricks in machine learning.**\n",
    "\n",
    "\n",
    "(Groups kernel?)\n",
    "\n",
    "As specified in the documentation, \"different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\"\n",
    "\n",
    "**SVC**: Support Vector Classifier (a type of svm)\n",
    "\n",
    "Kernel is a SVC parameter. Kernels available for SVC: \n",
    "* \"linear\" (Linear kernels generally draw a straight line)\n",
    "* \"poly\"\n",
    "* \"rbf\" (default)\n",
    "* \"sigmoid\"\n",
    "* \"precomputed\"\n",
    "* a callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# e.g.:\n",
    "clf = SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters in ML\n",
    "Arguments passed when you **create** your classifier. I.e. before fitting. Can have huge impact on the resulting decision boundary.\n",
    "\n",
    "Sample parameters for an SVM:\n",
    "* kernel\n",
    "* C\n",
    "* gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM C Parameter\n",
    "\n",
    "**C** controls the tradeoff between \n",
    "1. a smooth decision boundary and \n",
    "2. classifying training points correctly.\n",
    "\n",
    "**Tradeoff**: Complicated might not generalise as well. Straighter decision boundary might generalise better.\n",
    "\n",
    "Large C means larger error penalty -> More complicated boundary.\n",
    "-> **C for complicated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "C, gamma and kernel attributes all affect overfitting in SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Pros and Cons\n",
    "* works well in complicated domains where there is a clear margin of separation\n",
    "* doesn't work well with large datasets because training time is O(n^3) where n is the size of the dataset.\n",
    "* Don't work well with much noise (e.g. classes overlapping (or many features?)) -> Naive Bayes better.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

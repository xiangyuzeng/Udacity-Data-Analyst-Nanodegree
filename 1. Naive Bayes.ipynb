{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "// making great wine: Need great grapes for great wine. Need good data for good machine learning.\n",
    "\n",
    "### Applications\n",
    "1. Google: Search engine, speech recognition, language translation, self-driving cars\n",
    "2. Physics: particles datasets\n",
    "3. Choosing clothes, movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification\n",
    "Defn: Have examples where you know the correct answer for the examples. \n",
    "e.g. Self-driving cars emulate human behaviour.\n",
    "\n",
    "Application: Terrain classification problem. Desert terrain: slowing down at the appropriate time. Thousands of miles.\n",
    "\n",
    "\n",
    "**Example of supervised classification:**\n",
    "<table>\n",
    "<th>Supervised classification</th><th>Not supervised classification</th>\n",
    "<tr><td>Recognise someone in a picture from an album of tagged photos</td><td>Analyse bank data for weird-looking transactions and flag those for fraud</td></tr>\n",
    "<tr><td>Given someone's music choices and a bunch of features of that music, recommend a new song (Netflix, Amazon) </td><td>Cluster uDacity students into types based on learning styles</td></tr>\n",
    "</table>\n",
    "\n",
    "Explanation:\n",
    "* 2 Haven't defined what a weird-looking transaction is.\n",
    "* 4 Don't know what the types of students are or even number of types\n",
    "\n",
    "**Features** and **Labels**\n",
    "* e.g. Music: Tempo and intensity of a song. \n",
    "* e.g. Stanley Terrain classification. Features: Steepness and ruggedness of terrain.\n",
    "\n",
    "Visualise features via **Scatterplots**\n",
    "\n",
    "Machine learning algorithms define a **Decision Surface** that separates different classes.\n",
    "One one side they predict class A, and on the other side they predict class B.\n",
    "* Linear decision surfaces\n",
    "Bad ones may misclassify existing data, may come very close to misclassifying existing data.\n",
    "\n",
    "< Decision surface diagram >\n",
    "\n",
    "Machine learning algorithms transform:\n",
    "Data -> Decision surface (DS) that for all future cases can enable you to make a determination of which class the datapoint is in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sk-learn Naive Bayes to draw Decision Surfaces\n",
    "\n",
    "Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# sklearn.naive_bayes.GaussianNB\n",
    "\n",
    "# Creating training points\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Create classifier\n",
    "clf = GaussianNB()\n",
    "# Give classifier training data and it learns patterns\n",
    "# X are features, Y are labels.\n",
    "clf.fit(X, Y)\n",
    "GaussianNB()\n",
    "# Give classifier a new point and ask it to give a prediction\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second half of example in documentation\n",
    "\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "GaussianNB()\n",
    "print(clf_pf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: GaussianNB Deployment on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "#from udacityplots import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib \n",
    "matplotlib.use('agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.ioff()\n",
    "\n",
    "def prettyPicture(clf, X_test, y_test):\n",
    "    x_min = 0.0; x_max = 1.0\n",
    "    y_min = 0.0; y_max = 1.0\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    h = .01  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)\n",
    "\n",
    "    # Plot also the test points\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    plt.scatter(grade_sig, bumpy_sig, color = \"b\", label=\"fast\")\n",
    "    plt.scatter(grade_bkg, bumpy_bkg, color = \"r\", label=\"slow\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"bumpiness\")\n",
    "    plt.ylabel(\"grade\")\n",
    "\n",
    "    plt.savefig(\"test.png\")\n",
    "    \n",
    "import base64\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "def output_image(name, format, bytes):\n",
    "    image_start = \"BEGIN_IMAGE_f9825uweof8jw9fj4r8\"\n",
    "    image_end = \"END_IMAGE_0238jfw08fjsiufhw8frs\"\n",
    "    data = {}\n",
    "    data['name'] = name\n",
    "    data['format'] = format\n",
    "    data['bytes'] = base64.encodestring(bytes)\n",
    "    print image_start+json.dumps(data)+image_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prep terrain data\n",
    "\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "\n",
    "\n",
    "def makeTerrainData(n_points=1000):\n",
    "###############################################################################\n",
    "### make the toy dataset\n",
    "    random.seed(42)\n",
    "    grade = [random.random() for ii in range(0,n_points)]\n",
    "    bumpy = [random.random() for ii in range(0,n_points)]\n",
    "    error = [random.random() for ii in range(0,n_points)]\n",
    "    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]\n",
    "    for ii in range(0, len(y)):\n",
    "        if grade[ii]>0.8 or bumpy[ii]>0.8:\n",
    "            y[ii] = 1.0\n",
    "\n",
    "### split into train/test sets\n",
    "    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]\n",
    "    split = int(0.75*n_points)\n",
    "    X_train = X[0:split]\n",
    "    X_test  = X[split:]\n",
    "    y_train = y[0:split]\n",
    "    y_test  = y[split:]\n",
    "\n",
    "    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "\n",
    "#    training_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "#            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    test_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "#    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" Complete the code in ClassifyNB.py with the sklearn\n",
    "    Naive Bayes classifier to classify the terrain data.\n",
    "    \n",
    "    The objective of this exercise is to recreate the decision \n",
    "    boundary found in the lesson video, and make a plot that\n",
    "    visually shows the decision boundary \"\"\"\n",
    "\n",
    "\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from class_vis import prettyPicture, output_image\n",
    "from ClassifyNB import classify\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
    "### in together--separate them so we can give them different colors in the scatterplot,\n",
    "### and visually identify them\n",
    "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "\n",
    "\n",
    "def classify(features_train, labels_train):   \n",
    "    ### import the sklearn module for GaussianNB\n",
    "    ### create classifier\n",
    "    ### fit the classifier on the training features and labels\n",
    "    ### return the fit classifier\n",
    "    \n",
    "        \n",
    "    ### your code goes here!\n",
    "    \n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(features_train, labels_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "# You will need to complete this function imported from the ClassifyNB script.\n",
    "# Be sure to change to that code tab to complete this quiz.\n",
    "clf = classify(features_train, labels_train)\n",
    "\n",
    "\n",
    "\n",
    "### draw the decision boundary with the text points overlaid\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate classifier: Calculating NB Accuracy\n",
    "\n",
    "Metric: Accuracy = number of points classified correctly / total number of points in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# studentCode.py\n",
    "\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from classify import NBAccuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "def submitAccuracy():\n",
    "    accuracy = NBAccuracy(features_train, labels_train, features_test, labels_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exercise\n",
    "\n",
    "def NBAccuracy(features_train, labels_train, features_test, labels_test):\n",
    "    \"\"\" compute the accuracy of your Naive Bayes classifier \"\"\"\n",
    "    ### import the sklearn module for GaussianNB\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    ### create classifier\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    ### fit the classifier on the training features and labels\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    ### use the trained classifier to predict labels for the test features\n",
    "    pred = clf.predict(features_test)\n",
    "\n",
    "\n",
    "    ### calculate and return the accuracy on the test data\n",
    "    ### this is slightly different than the example, \n",
    "    ### where we just print the accuracy\n",
    "    ### you might need to import an sklearn module\n",
    "    accuracy = clf.score(features_test, labels_test)\n",
    "    return accuracy\n",
    "\n",
    "# 0.884"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Data\n",
    "\n",
    "Train and test on different sets of data. e.g. 10% of data for testing.\n",
    "Else can get 100% accuracy with no idea of how to generalise something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule\n",
    "\n",
    "Cancer example P(C) = 0.01\n",
    "\n",
    "**Sensitivity of a Test (Size?)**: 90% it is positive if you have C.\n",
    "**Specitivity of a Test (Power?)** 90% it is negative if you don't have C.\n",
    "\n",
    "Q: the prior probability of cancer is 1%, and a sensitivity and specificity are 90%, what's the probability that someone with a positive cancer test actually has the disease?\n",
    "\n",
    "(8 %)\n",
    "\n",
    "**Prior probability * Test Evidence (incorporates into former) -> Posterior Probability**\n",
    "\n",
    "### Exploration\n",
    "\n",
    "**Prior**\n",
    "\n",
    "$P(C) = 0.01$\n",
    "\n",
    "$P(Pos|C) = 0.9$\n",
    "\n",
    "$P(Neg|\\neg C) = 0.9$\n",
    "\n",
    "**Joint probability**\n",
    "\n",
    "$P(C,Pos) = P(C) * P(Pos|C)$\n",
    "\n",
    "$P(\\neg C,Pos) = P(\\neg C) * P(Pos|\\neg C)$\n",
    "\n",
    "i.e. calculating absolute area in regions. But they don't add up to one.\n",
    "\n",
    "**Normaliser**\n",
    "\n",
    "$P(Pos) = P(C, Pos) + P(\\neg C, Pos)$\n",
    "\n",
    "**Posterior**\n",
    "\n",
    "$P(C|Pos) = 0.0833$\n",
    "\n",
    "$P(\\neg C|Pos) = 0.9167$\n",
    "\n",
    "Posteriors sum to 1.\n",
    "\n",
    "(Diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes e.g. Text Learning\n",
    "\n",
    "Q: Given email text and probabilities with which Chris and Sara use each word (total 3 words can be used), who is more likely to have sent the email?\n",
    "\n",
    "Suppose prior $P(Chris) = P(Sara) = 0.5$\n",
    "\n",
    "Diagram\n",
    "\n",
    "Then given text: \"LIFE DEAL\"\n",
    "\n",
    "Chris: $0.1*0.8*0.5 = 0.04$\n",
    "\n",
    "Sara: $0.3*0.2*.0.5 = 0.03$\n",
    "\n",
    "P(Chris | \"LIFE DEAL\") = 4/7\n",
    "\n",
    "P(Sara | \"LIFE DEAL\") = 3/7\n",
    "\n",
    "**Why is Naive Bayes Naive?**\n",
    "\n",
    "* Labels are hidden. You only get to see things these things do. \n",
    "* Every word, say, gives you evidence as to whether it's person A or person B. \n",
    "* You multiply evidence for all the words you see and the prior, and the products give you the ratio as to whether it's from A or B.\n",
    "\n",
    "Let's you identify from a text source whether label A or label B is more likely.\n",
    "\n",
    "Naive because it ignores **word order**. Only looks at word frequency.\n",
    "\n",
    "**Advantages**\n",
    "* Easy to implement\n",
    "* Efficient\n",
    "\n",
    "\"One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts.\"\n",
    "\n",
    "**Disadvantages**\n",
    "* Can break. \n",
    "* Phrases that encompass different words and have distinct meanings don't work well: when people first searched for Chicago Bulls on Google, they were shown many images of the city Chicago and images of bulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from Mini-Project: Identifying author of a piece of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes predicts faster than it trains."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

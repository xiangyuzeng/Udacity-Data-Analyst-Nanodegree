{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Regression\n",
    "\n",
    "This section is concerned with **continuous supervised learning**. That is, the output is continuous. So far the output tends to be binary (slow or fast), which is discrete.\n",
    "\n",
    "Continuous or discrete?\n",
    "- Person who wrote email out of 100 people: Unordered so discrete classifier usually better even if the categories involve numbers.\n",
    "- Phone numbes: Discrete. There is some order (region codes) but generally 024-3111 and 024-3112 are not related.\n",
    "**Continuous: Has order.**\n",
    "\n",
    "Generally, result of supervised learning algorithm on continuous output is expressed as\n",
    "[img](images/6-21.png)\n",
    "\n",
    "TARGET = SLOPE * INPUT + INTERCEPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def studentReg(ages_train, net_worths_train):\n",
    "    ### import the sklearn regression module, create, and train your regression\n",
    "    ### name your regression reg\n",
    "    \n",
    "    ### your code goes here!\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    reg = LinearRegression()\n",
    "    reg = reg.fit(ages_train, net_worths_train)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import numpy\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from studentRegression import studentReg\n",
    "from class_vis import prettyPicture, output_image\n",
    "\n",
    "from ages_net_worths import ageNetWorthData\n",
    "\n",
    "ages_train, ages_test, net_worths_train, net_worths_test = ageNetWorthData()\n",
    "\n",
    "reg = studentReg(ages_train, net_worths_train)\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(ages_train, net_worths_train, color=\"b\", label=\"train data\")\n",
    "plt.scatter(ages_test, net_worths_test, color=\"r\", label=\"test data\")\n",
    "# Draw line\n",
    "plt.plot(ages_test, reg.predict(ages_test), color=\"black\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"ages\")\n",
    "plt.ylabel(\"net worths\")\n",
    "\n",
    "\n",
    "plt.savefig(\"test.png\")\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Regression Metrics**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Net worth prediction: \", reg.predict([27]))\n",
    "print(\"Slope: \", reg.coef_)\n",
    "print(\"Intercept: \", reg.intercept_)\n",
    "\n",
    "# Overfitting -> lower score on test dataset\n",
    "# Note reg is already fitted.\n",
    "print(\"R-squared score (test): \", reg.score(ages_test, net_worths_test))\n",
    "\n",
    "print(\"R-squared score (train): \", reg.score(ages_train, net_worths_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Errors\n",
    "\n",
    "Error = actual value - predicted value\n",
    "\n",
    "Want to **minimise sum of squared errors**.\n",
    "\n",
    "$$min\\sum(actual - predicted)$$\n",
    "\n",
    "over the training set.\n",
    "\n",
    "[img](images/6-40.png)\n",
    "\n",
    "[img](images/6-41.png)\n",
    "\n",
    "Several algorithms:\n",
    "- Ordinary Last Squares\n",
    "- Gradient Descent\n",
    "\n",
    "Q: Why minimise the sum of SQUARED errors vs abs?\n",
    "\n",
    "1. Best fit line in 06-45: analogous to **SVM large margin**.\n",
    "If there are the same number of points above vs below the line for multiple lines, **there can be multiple lines that minimise the absolute errors**. E.g. if lines are parallel cause the additional error for one point above the line from shifting the line within these constraints cancels with the reduction in error for a point below the line.\n",
    "\n",
    "2. Computationally easier than absolute values\n",
    "\n",
    "### Problems with SSE\n",
    "1. More data -> Larger SSE. (even though not necc worse fit)\n",
    "\n",
    "## R^2\n",
    "\n",
    "Answers the question 'How much of the change in the output is explained by the change in my input?'\n",
    "\n",
    "In range [0,1]. (Dunno if inclusive) Larger r^2 means better fit.\n",
    "\n",
    "It is independent of the number of training points.\n",
    "\n",
    "## Visualise data to see if it's good for linear regression\n",
    "- Alternatives: Fitting data piecewise\n",
    "\n",
    "[img](images/6-53.png)\n",
    "\n",
    "[img](images/6-54.png)\n",
    "\n",
    "## Comparing Classification and Regression\n",
    "\n",
    "[img](images/6-55.png)\n",
    "\n",
    "## Multivariate regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

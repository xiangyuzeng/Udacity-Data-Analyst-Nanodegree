{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Supervised Classification Algorithm 3/3\n",
    "\n",
    "// Kernel trick to go from linear to non-linear classification\n",
    "\n",
    "### Non-linear decision surfaces with Decision Trees\n",
    "e.g. Wind-surfing: Need wind AND sun. -> Not linearly separable.\n",
    "\n",
    "Decision trees allow you to ask multiple linear questions one after another. E.g. Is it windy? (YES or NO) -> Is it sunny?\n",
    "\n",
    "[3-05.png](images/3-05.png)\n",
    "\n",
    "### Constructing a Decision Tree\n",
    "Axis-parallel decision lines\n",
    "\n",
    "[3-13.png](images/3-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding a Decision Tree\n",
    "`sk-learn` Decision Trees can also be used for regression.\n",
    "Cost of using the tree is logarithmic in the size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]] [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0,0], [1,1]]\n",
    "Y = [0,1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X,Y)\n",
    "print(clf.predict_proba([[2.,2.]]), clf.predict([[2.,2.]]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" lecture and example code for decision tree unit \"\"\"\n",
    "\n",
    "import sys\n",
    "from class_vis import prettyPicture, output_image\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the classify() function in classifyDT is where the magic\n",
    "### happens--fill in this function in the file 'classifyDT.py'!\n",
    "\n",
    "def classify(features_train, labels_train):\n",
    "    \n",
    "    ### your code goes here--should return a trained decision tree classifer\n",
    "    from sklearn import tree\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(features_train, labels_train)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "\n",
    "clf = classify(features_train, labels_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy: %d\" % metrics.accuracy_score(clf.predict(features_test), labels_test))\n",
    "\n",
    "#### grader code, do not modify below this line\n",
    "\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image](images/3-15.png)\n",
    "\n",
    "Distinctive, slicy decision boundaries. Can have islands.\n",
    "\n",
    "There are signs of overfitting.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "###  `min_samples_split`\n",
    "Decide if you're going to keep splitting. -> If there are enough samples left. Default is 2. \n",
    "* Small `min_samples_split` -> overfitting.\n",
    "\n",
    "[image](images/3-21.png)\n",
    "\n",
    "mss=2 gives lower accuracy than mss=50 for our examples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "\n",
    "\n",
    "########################## DECISION TREE #################################\n",
    "\n",
    "\n",
    "### your code goes here--now create 2 decision tree classifiers,\n",
    "### one with min_samples_split=2 and one with min_samples_split=50\n",
    "### compute the accuracies on the testing data and store\n",
    "### the accuracy numbers to acc_min_samples_split_2 and\n",
    "### acc_min_samples_split_50, respectively\n",
    "\n",
    "def return_accuracy_for_mss(mss_value):\n",
    "    clf = tree.DecisionTreeClassifier(min_samples_split=mss_value)\n",
    "    clf = clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    return metrics.accuracy_score(pred, labels_test)\n",
    "\n",
    "acc_min_samples_split_2 = return_accuracy_for_mss(2)\n",
    "acc_min_samples_split_50 = return_accuracy_for_mss(50)\n",
    "\n",
    "def submitAccuracies():\n",
    "  return {\"acc_min_samples_split_2\":round(acc_min_samples_split_2,3),\n",
    "          \"acc_min_samples_split_50\":round(acc_min_samples_split_50,3)}\n",
    "\n",
    "# Returns:\n",
    "# {\"message\": \"{'acc_min_samples_split_50': 0.912, 'acc_min_samples_split_2': 0.908}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Measure of impurity in a bunch of examples.\n",
    "Controls how a decision tree decides where to split the data\n",
    "\n",
    "E.g.: self-driving car:\n",
    "\n",
    "Q: are you going fast?\n",
    "\n",
    "[image](images/3-24.png)\n",
    "\n",
    "Can split based on bumpiness or speed limit.\n",
    "\n",
    "[image](images/3-25.png)\n",
    "\n",
    "Splitting based on Speed Limit has more purity, whereas LHS has contamination from some fast points.\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "entropy = $$\\sum-p_ilog_2(p_i)$$\n",
    "\n",
    "Sum over all classes. $p_i$ is the fraction of examples in class i.\n",
    "\n",
    "Intuition:\n",
    "\n",
    "All examples are the same class. Entropy = 0.\n",
    "Examples are evenly split between classes: Entropy = 1.0 (Max)\n",
    "\n",
    "[image](images/3-36.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating entropy of node\n",
    "\n",
    "import math\n",
    "-0.5*math.log(0.5, 2)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain: Entropy and Decision Trees\n",
    "\n",
    "[image](images/3-46.png)\n",
    "\n",
    "Weighted according to number of records in each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy of left child node if we split by grade\n",
    "-2/3*math.log(2/3,2)-1/3*math.log(1/3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31127812445913283"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information gain\n",
    "1 - (3/4*0.9182958340544896)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image](images/3-50.png)\n",
    "\n",
    "Next, calc information gain when we split by bumpiness.\n",
    "\n",
    "[image](images/3-54.png)\n",
    "\n",
    "Information gain is 0 -> Not where we want to start splitting our sample if we want to build a decision tree.\n",
    "\n",
    "Split on speed limit: Get perfect purity of data and information gain is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat to calculation:\n",
    "Splitting parameter: sklearn default is `gini`. Need to specify `entropy` for information gain by hand.\n",
    "\n",
    "## Bias and variance tradeoff\n",
    "\n",
    "**High bias** ML algorithm is one that practically ignores the data and has no capacity to learn anything.\n",
    "\n",
    "Other extreme is **high variance**: Reacts poorly to situations it hasn't seen before because it only repeats what it's seen before, doesn't have the bias to generalise.\n",
    "\n",
    "## Pros and Cons to Decision Trees\n",
    "\n",
    "Easy to use, graphically allow you to interpret data well\n",
    "\n",
    "BUT\n",
    "* prone to overfitting, especially if data has many features -> stop growth of tree at appropriate time\n",
    "* Can build bigger classifiers out of decision trees through ensemble methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Analysing Data\n",
    "\n",
    "Exploring data using the MongoDB aggregation framework: initial analysis and additional cleaning. Used to create data processing pipelines.\n",
    "\n",
    "e.g. Twitter Data Set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Twitter Data Schema\n",
    "\n",
    "{\n",
    "    \"_id\" : ObjectId(\"4380oueut34.\"),\n",
    "    \"text\" : \"Hello\",\n",
    "    \"entities\" : {\n",
    "        \"user_mentions\" : [\n",
    "            {\n",
    "                \"screen_name\" : \"somebody_else\",\n",
    "            }\n",
    "        ],\n",
    "        \"urls\" : [ ],\n",
    "        \"hashtags\" : [ ]\n",
    "    },\n",
    "    \"user\" : {\n",
    "        \"friends_count\" : 544,\n",
    "        \"screen_name\" : \"somebody\",\n",
    "        \"followers_count\" : 100,\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of questions: Understand behaviour of users and networks involved.\n",
    "\n",
    "Aggregation Framework provides a powerful tool for analysing data.\n",
    "E.g.: Determine which user has produced the most tweets. \n",
    "Process:\n",
    "1. Group tweets by user\n",
    "2. Count each user's tweets\n",
    "3. Sort into descending order (of number of tweets)\n",
    "4. Select user at the top (the one with most tweets)\n",
    "\n",
    "Aggregation queries in MongoDB issued using 'aggregate', done using a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.twitter\n",
    "\n",
    "def most_tweets():\n",
    "    # Issue aggregation query\n",
    "    result = db.tweets.aggregate([\n",
    "            # For user subdocument, I want the screen name field\n",
    "            # \"$user.screen_name\" don't make it a string. Not an operator.\n",
    "            # Want value of user.screen_name\n",
    "            { \"$group\" : { \"_id\" : \"$user.screen_name\",\n",
    "                          # Accumulator operator \"$sum\": For all docs that have the same value for _id,\n",
    "                          # Increment count by 1.\n",
    "                           \"count\" : { \"$sum\" : 1 } } },\n",
    "            # Sort docs passed into this stage (output of \"$group\")\n",
    "            # based on the count in descending order\n",
    "            { \"$sort\" : { \"count\" : -1} } ])\n",
    "    return result\n",
    "\n",
    "if __name__ = \"__main__\":\n",
    "    result = most_tweets()\n",
    "    pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# in Terminal\n",
    "# pipe command into less to see top of results produced\n",
    "python file_name | less\n",
    "\n",
    "<u'ok' : 1.0,\n",
    " u'result': [{u'_id': u'behcolin', u'count': 8},\n",
    "             ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Pipline \n",
    "\n",
    "* diagram\n",
    "* e.g. \"\\$group\" -> \"\\$sort\" \n",
    "* Collection fed into group stage. Finds tweets per user and accumulates them.\n",
    "* Depending on which operator is used in a given stage, stage may be reshaping data. Collection of tweets have dozens of fields, putting through \"\\$group\" stage turns it into data with 2 fields.\n",
    "* Use aggregation operators to produce stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "The tweets in our twitter collection have a field called \"source\". This field describes the application\n",
    "that was used to create the tweet. Following the examples for using the $group operator, your task is \n",
    "to modify the 'make-pipeline' function to identify most used applications for creating tweets. \n",
    "As a check on your query, 'web' is listed as the most frequently used application.\n",
    "'Ubertwitter' is the second most used. The number of counts should be stored in a field named 'count'\n",
    "(see the assertion at the end of the script).\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation pipeline\n",
    "that can be passed to the MongoDB aggregate function. As in our examples in this lesson, the aggregation \n",
    "pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. \n",
    "If you want to run this code locally on your machine, you have to install MongoDB, \n",
    "download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset \n",
    "used in examples in this lesson. \n",
    "If you attempt some of the same queries that we looked at in the lesson examples,\n",
    "your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [{\"$group\" : {\"source\" : \"$source\",\n",
    "                             \"count\" : {\"$sum\" : 1} } },\n",
    "                {\"$sort\" : {\"count\" : -1} } ]\n",
    "    return pipeline\n",
    "\n",
    "def tweet_sources(db, pipeline):\n",
    "    return [doc for doc in db.tweets.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = tweet_sources(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert result[0] == {u'count': 868, u'_id': u'web'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Operators: An overview\n",
    "\n",
    "Previously introduced **group** and **sort**.\n",
    "\n",
    "### 1. \\$project\n",
    "Reshaping data for the next stage.\n",
    "* e.g. Selecting which fields you are interested in, regardless of where they are nested.\n",
    "\n",
    "Functions:\n",
    "* Include fields from  the original document\n",
    "* Insert computed fileds (e.g. produce ratio based on two numeeric fields)\n",
    "* Rename fields\n",
    "* Create fields that hold subdocuments \n",
    "\n",
    "### 2. \\$match\n",
    "Filters documents\n",
    "\n",
    "### 3. \\$skip\n",
    "Skip over a certain number of documents at the beginning of the input set of documents.\n",
    "\n",
    "### 4. \\$limit\n",
    "\n",
    "Limit is opposite of skipe. Limit to first 3 documents of input to next stage, say.\n",
    "\n",
    "### 5. \\$unwind\n",
    "\n",
    "(Recall fields can have arrays as values.)\n",
    "* For every value of an array field, it will create an instance of the document containing that array field for every value in the array.\n",
    "* Can run groupby in next stage of the pipeline\n",
    " where we care about individual values and group based on those values.\n",
    " * e.g. group based on hashtags included in tweets. \n",
    " \n",
    "For more operators see [Project operator documentation](http://docs.mongodb.org/manual/reference/operator/aggregation/project/#pipe._S_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question to work on\n",
    "Who has the highest followers to friends (followers to following) ratio? (Twitter dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.examples\n",
    "\n",
    "def highest_ratio():\n",
    "    result = db.tweets.aggregate([\n",
    "            # Specify constraints on type of doc we want to let through match stage.\n",
    "            # I.e. those who have both a friends and followers count >0\n",
    "            # Specifying field, inequality operator, value 0.\n",
    "            {\"$match\": {\"user.friends_count\": {\"$gt\": 0},\n",
    "                        \"user.followers_count\": {\"$gt\": 0}}},\n",
    "            # Ratio field has value followers_count/friends_count\n",
    "            {\"$project\": {\"ratio\": {\"$divide\": [\"$user.followers_count\",\n",
    "                                                \"$user.friends_count\"]},\n",
    "                                    # Dollar sign because not string literal\n",
    "                                    \"screen_name\": \"$user.screen_name\"}},\n",
    "            # Here, docs will have precisely two fields: screen_name and ratio.\n",
    "            # Sort in descending ordor of ratio (descending -> -1)\n",
    "            {\"$sort\": {\"ratio\": -1}},\n",
    "            # We only want the top (largest) entry\n",
    "            {\"$limit\": 1}\n",
    "        ])\n",
    "    return result\n",
    "\n",
    "# result = highest_ratio()\n",
    "\n",
    "# Returns array-valued field with three fields '_id', 'ratio', 'screen_name'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz: Using match and project\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Write an aggregation query to answer this question:\n",
    "\n",
    "Of the users in the \"Brasilia\" timezone who have tweeted 100 times or more,\n",
    "who has the largest number of followers?\n",
    "\n",
    "The following hints will help you solve this problem:\n",
    "- Time zone is found in the \"time_zone\" field of the user object in each tweet.\n",
    "- The number of tweets for each user is found in the \"statuses_count\" field.\n",
    "  To access these fields you will need to use dot notation (from Lesson 4)\n",
    "- Your aggregation query should return something like the following:\n",
    "{u'ok': 1.0,\n",
    " u'result': [{u'_id': ObjectId('52fd2490bac3fa1975477702'),\n",
    "                  u'followers': 2597,\n",
    "                  u'screen_name': u'marbles',\n",
    "                  u'tweets': 12334}]}\n",
    "Note that you will need to create the fields 'followers', 'screen_name' and 'tweets'.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson,\n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code\n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used \n",
    "in examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    \"\"\"\n",
    "    Of the users in the \"Brasilia\" timezone who have tweeted 100 times or more,\n",
    "    who has the largest number of followers?\n",
    "    \"\"\"\n",
    "    pipeline = [\n",
    "        # Users in \"Brasilia\" timezone who have tweeted 100 times or more\n",
    "        {\"$match\": {\"user.time_zone\": \"Brasilia\",\n",
    "                    \"user.statuses_count\": {\"$gte\": 100}}},\n",
    "        # Number of followers\n",
    "        {\"$project\": {\"followers\": \"$user.followers_count\",\n",
    "                      \"screen_name\": \"$user.screen_name\",\n",
    "                      \"tweets\": \"$user.statuses_count\"}},\n",
    "        # Sort by number of followers (Descending)\n",
    "        {\"$sort\": {\"followers\": -1}},\n",
    "        # Only highest (first)\n",
    "        {\"$limit\": 1}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def highest_ratio():\n",
    "    result = db.tweets.aggregate([\n",
    "            # Specify constraints on type of doc we want to let through match stage.\n",
    "            # I.e. those who have both a friends and followers count >0\n",
    "            # Specifying field, inequality operator, value 0.\n",
    "            {\"$match\": {\"user.friends_count\": {\"$gt\": 0},\n",
    "                        \"user.followers_count\": {\"$gt\": 0}}},\n",
    "            # Ratio field has value followers_count/friends_count\n",
    "            {\"$project\": {\"ratio\": {\"$divide\": [\"$user.followers_count\",\n",
    "                                                \"$user.friends_count\"]},\n",
    "                                    # Dollar sign because not string literal\n",
    "                                    \"screen_name\": \"$user.screen_name\"}},\n",
    "            # Here, docs will have precisely two fields: screen_name and ratio.\n",
    "            # Sort in descending ordor of ratio (descending -> -1)\n",
    "            {\"$sort\": {\"ratio\": -1}},\n",
    "            # We only want the top (largest) entry\n",
    "            {\"$limit\": 1}\n",
    "        ])\n",
    "    return result\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.tweets.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 1\n",
    "    assert result[0][\"followers\"] == 17209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on \\$unwind operator\n",
    "\n",
    " E.g. who included the most user mentions? (included inside the entities subdocument \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Give me back docs where user mentions field are of length 3.\n",
    "> db.tweets.find( { \"entities.user_mentions\" : {\"$size\" : 3 } } )\n",
    "# Interested in entities.user_mentions.screen_name\n",
    "\n",
    "# Look through all tweets and count the number of user mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.examples\n",
    "\n",
    "def user_mentions():\n",
    "    result = db.tweets.aggregate([\n",
    "            {\"$unwind\": \"$entities.user_mentions\"},\n",
    "            # Group docs that passed through first stage by user who created the tweet\n",
    "            # Produce a count field that increments counter each time it sees the document that has the same screen name\n",
    "            # Counts all mentions, not unique mentions. I.e. if I mention @a twice in my tweet, it's counted twice.\n",
    "            {\"$group\": {\"_id\": \"$user.screen_name\",\n",
    "                        \"count\": {\"$sum\": 1}}},\n",
    "            # Sort in descending ordor of count (descending -> -1)\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            # We only want the top (largest) entry\n",
    "            {\"$limit\": 1}\n",
    "        ])\n",
    "    return result\n",
    "\n",
    "# High speed of execution of queries because this functionality is fundamental to server itself.\n",
    "\n",
    "# result = user_mentions()\n",
    "# pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz: Using unwind\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "For this exercise, let's return to our cities infobox dataset. The question we would like you to answer\n",
    "is as follows:  Which region or district in India contains the most cities? (Make sure that the count of\n",
    "cities is stored in a field named 'count'; see the assertions at the end of the script.)\n",
    "\n",
    "As a starting point, use the solution for the example question we looked at -- \"Who includes the most\n",
    "user mentions in their tweets?\"\n",
    "\n",
    "One thing to note about the cities data is that the \"isPartOf\" field contains an array of regions or \n",
    "districts in which a given city is found. See the example document in Instructor Comments below.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation pipeline \n",
    "that can be passed to the MongoDB aggregate function. As in our examples in this lesson, the aggregation \n",
    "pipeline should be a list of one or more dictionary objects. Please review the lesson examples if you \n",
    "are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the cities collection used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results may be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "\n",
    "\n",
    "def make_pipeline():\n",
    "    \"\"\"\n",
    "    Which region or district in India contains the most cities? (Make sure that the count of\n",
    "    cities is stored in a field named 'count'; see the assertions at the end of the script.)\n",
    "    \"\"\"\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        # We want only entries with country = India\n",
    "        {\"$match\": {\"country\": \"India\"}},\n",
    "        # Unwind to separate isPartOf (regions)\n",
    "        {\"$unwind\": \"$isPartOf\"},\n",
    "        # Group by region (isPartOf) and count\n",
    "        {\"$group\": {\"_id\": \"$isPartOf\",\n",
    "                    \"count\": {\"$sum\": 1}}},\n",
    "        # Sort by count (descending)\n",
    "        {\"$sort\": {\"$count\": -1}},\n",
    "        # Return region with highest count\n",
    "        {\"$limit\": 1}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    print \"Printing the first result:\"\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert result[0][\"_id\"] == \"Uttar Pradesh\"\n",
    "    assert result[0][\"count\"] == 623\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Returned \n",
    "pymongo.errors.OperationFailure: exception: FieldPath field names may not start with '$'.\n",
    "\n",
    "with\n",
    "\n",
    "pipeline = [\n",
    "        # We want only entries with country = India\n",
    "        {\"$match\": {\"country\": \"India\"}},\n",
    "        # Unwind to separate isPartOf (regions)\n",
    "        {\"$unwind\": \"$isPartOf\"},\n",
    "        # Group by region (isPartOf) and count\n",
    "        {\"$group\": {\"_id\": \"$isPartOf\",\n",
    "                    \"count\": {\"$sum\": 1}}},\n",
    "        # Sort by count (descending)\n",
    "        {\"$sort\": {\"$count\": -1}},\n",
    "        # Return region with highest count\n",
    "        {\"$limit\": 1}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\$group (more detail)\n",
    "\n",
    "Aggregates input in some way based on operator specified.\n",
    "\n",
    "* \\$sum\n",
    "* \\$first (Selects first documented group)\n",
    "* \\$last\n",
    "* \\$max\n",
    "* \\$min\n",
    "* \\$avg\n",
    "\n",
    "Question to investigate: Calculate average number of retweets for any tweet based on the hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.examples\n",
    "\n",
    "def hashtag_retweet_avg():\n",
    "    result = db.tweets.aggregate([\n",
    "            {\"$unwind\": \"$entities.hashtags\"},\n",
    "            # Aggregate based on hashtag itself\n",
    "            {\"$group\": {\"_id\": \"$entities.hashtags.text\",\n",
    "                        \"retweet_avg\": {\"$avg\": \"$retweet_count\"}\n",
    "                       }},\n",
    "            # Sort in descending ordor of count (descending -> -1)\n",
    "            {\"$sort\": {\"retweet_avg\": -1}}\n",
    "        ])\n",
    "    return result\n",
    "\n",
    "# result = user_mentions()\n",
    "# pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More group operators. These deal with arrays.\n",
    "*\\$push\n",
    "*\\$addToSet: Adds values to an array by treating it as a set, i.e. won't add same value more than once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.examples\n",
    "\n",
    "def unique_hashtags_by_user():\n",
    "    result = db.tweets.aggregate([\n",
    "            {\"$unwind\": \"$entities.hashtags\"},\n",
    "            # Aggregate based on hashtag itself\n",
    "            {\"$group\": {\"_id\": \"$user.screen_name\",\n",
    "                        \"unique_hashtags\": {\n",
    "                        # Each unique hashtag added precisely once\n",
    "                        \"$addToSet\": \"$entities.hashtags.text\"}\n",
    "                       }},\n",
    "            # Sort in descending ordor of count (descending -> -1)\n",
    "            {\"$sort\": {\"_id\": -1}}\n",
    "        ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\$push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using push\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "$push is similar to $addToSet. The difference is that rather than accumulating only unique values \n",
    "it aggregates all values into an array.\n",
    "\n",
    "Using an aggregation query, count the number of tweets for each user. In the same $group stage, \n",
    "use $push to accumulate all the tweet texts for each user. Limit your output to the 5 users\n",
    "with the most tweets. \n",
    "Your result documents should include only the fields:\n",
    "\"_id\" (screen name of user), \n",
    "\"count\" (number of tweets found for the user),\n",
    "\"tweet_texts\" (a list of the tweet texts found for the user).  \n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson, \n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        # Count number of tweets for each user: group tweets by user\n",
    "        {\"$group\": {\"_id\": \"$user.screen_name\", \n",
    "                    \"count\": {\"$sum\": 1},\n",
    "                    # Accumulate tweet texts\n",
    "                    \"tweet_texts\": {\"$push\": \"$text.text\"}}},\n",
    "        # Limit to 5 users with most tweets\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": 5}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.twitter.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 5\n",
    "    assert result[0][\"count\"] > result[4][\"count\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple stages using a given operator\n",
    "\n",
    "Question: Who has mentioned the most (unique users)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.examples\n",
    "\n",
    "def unique_user_mentions():\n",
    "    result = db.tweets.aggregate([\n",
    "            {\"$unwind\": \"$entities.user_mentions\"},\n",
    "            # Aggregating on user's screen name. Use $addToSet of user mentioned.\n",
    "            # i.e. unique set of users mentioned in tweets produced by each user.\n",
    "            {\"$group\": {\n",
    "                    \"_id\": \"$user.screen_name\",\n",
    "                    # mset stands for 'mentions set'\n",
    "                    \"mset\": {\n",
    "                        \"$addToSet\": \"$entities.user_mentions.screen_name\"\n",
    "                    }}},\n",
    "            # Now we have a list of unique users. Haven't counted them yet.\n",
    "            # Unwind generates one doc for every unique user mentioned per user\n",
    "            {\"$unwind\": \"$mset\"},\n",
    "            # Calculate count that we want. $_id directs to same _id as before, i.e. $user.screen_name\n",
    "            {\"$group\": {\"_id\": \"$_id\", \"count\": {\"$sum\": 1}}}\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            {\"$limit\": 10}\n",
    "        ])\n",
    "    return result\n",
    "\n",
    "# High speed of execution of queries because this functionality is fundamental to server itself.\n",
    "\n",
    "# result = user_mentions()\n",
    "# pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample document for following quiz:\n",
    "\n",
    "{\n",
    "    \"_id\" : ObjectId(\"52fe1d364b5ab856eea75ebc\"),\n",
    "    \"elevation\" : 1855,\n",
    "    \"name\" : \"Kud\",\n",
    "    \"country\" : \"India\",\n",
    "    \"lon\" : 75.28,\n",
    "    \"lat\" : 33.08,\n",
    "    \"isPartOf\" : [\n",
    "        \"Jammu and Kashmir\",\n",
    "        \"Udhampur district\"\n",
    "    ],\n",
    "    \"timeZone\" : [\n",
    "        \"Indian Standard Time\"\n",
    "    ],\n",
    "    \"population\" : 1140\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz: Same Operator\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "In an earlier exercise we looked at the cities dataset and asked which region in India contains \n",
    "the most cities. In this exercise, we'd like you to answer a related question regarding regions in \n",
    "India. What is the average city population for a region in India? Calculate your answer by first \n",
    "finding the average population of cities in each region and then by calculating the average of the \n",
    "regional averages.\n",
    "\n",
    "Hint: If you want to accumulate using values from all input documents to a group stage, you may use \n",
    "a constant as the value of the \"_id\" field. For example, \n",
    "    { \"$group\" : {\"_id\" : \"India Regional City Population Average\",\n",
    "      ... }\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson, \n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used \n",
    "in examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    \"\"\"\n",
    "    What is the average city population for a region in India? Calculate your answer by first \n",
    "    finding the average population of cities in each region and then by calculating the average of the \n",
    "    regional averages.\n",
    "    \"\"\"\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        # Unwind by \"isPartOf\"\n",
    "        {\"$unwind\": \"$isPartOf\"},\n",
    "        # Group by region (\"isPartOf\") and calculate average pop of cities in each region\n",
    "        {\"$group\": {\"_id\": \"$isPartOf\",\n",
    "                    \"average_city_pop\": {\"$avg\": \"$population\"}}},\n",
    "        # Calculate average of all regional averages\n",
    "        {\"$group\": {\"_id\": \"Average city pop for a region in India\",\n",
    "                    \"avg\": {\"$avg\": \"$average_city_pop\"}}}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    assert len(result) == 1\n",
    "    # Your result should be close to the value after the minus sign.\n",
    "    assert abs(result[0][\"avg\"] - 196025.97814809752) < 10 ** -8\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't know why code above is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Performance: Driven by 'Use Indexes or Not'\n",
    "\n",
    "Basics of indexing\n",
    "\n",
    "DB stores data in large files on disk. \n",
    "Pull out document using a query. By default DB scans through the entire collection to find the data. (Table scan in rel DB or collection scan in MongDB.) -> Death for performance.\n",
    "\n",
    "Instead, create an index or more than one index.\n",
    "\n",
    "**How it works**\n",
    "\n",
    "If something is ordered, can use binary search to do it.\n",
    "Specify a key.\n",
    "Index is an ordered key.\n",
    "Not stored linearly in MongoDB, stored using a B tree. But conceptually like the picture.\n",
    "\n",
    "Indexes are ordered lists of keys. You can have just one or you can construct one out of e.g. 3 keys (Hashtag, date, username). Order matters because conceptually the index is built by level. Here, within hashtags you have dates, within dates you have users.\n",
    "\n",
    "(Diagram 2)\n",
    "\n",
    "In order for MongoDB to use an index, you have to give it a leftmost set of items. In above example, just giving a date won't be useful.\n",
    "\n",
    "Every time you insert something into the database, the index needs to be updated so it takes time. Need to take this into consideration when designing indices.\n",
    "\n",
    "### Using indexes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> use osm\n",
    "switched to db osm\n",
    "# find everything\n",
    "> db.nodes.find().pretty()\n",
    "# Note some docs have a tg field, i.e. a tag. Array-valued tag fields, each array  field is a subdocument.\n",
    "# Over 7MM docs in this collection.\n",
    "# Q: how long does a query take to come back without an index?\n",
    "> db.nodes.find({\"tg\":{\"k\": \"name\", \"v\": \"Giordanos\"}}).pretty()\n",
    "# Takes a few seconds for a single query. But in most applications, doing many queries (up to 100,000s)\n",
    "# Make it the case that there is an index on field \"tg\" in the collection \"nodes\".\n",
    "> db.nodes.ensureIndex({\"tg\": 1})\n",
    "# Takes a couple of minutes. \n",
    "> db.nodes.find().pretty()\n",
    "# Now, response is immediate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on indexing: MongoDB University or MongoDB documentation on Indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Indexes\n",
    "Queries for location near a location, e.g. looking for a nearby cafe.\n",
    "\n",
    "Here look at 2D. MongoDB also supports spherical geospatial indexes.\n",
    "\n",
    "Find other items that are close to query location (x,y).\n",
    "\n",
    "Three steps:\n",
    "1. Location: [x,y] (stored as an array.)\n",
    "2. Call \"ensureIndex\" to create an index on this field. i.e. ensureIndex({'location':...})\n",
    "3. Query using \\$near operator.\n",
    "\n",
    "In PyMongo, pass a list of tuples for ensure_index\n",
    "self.client.osm.nodes.ensure_index([('loc', pymongo.GEO2D)]}\n",
    "\n",
    "Treat pymongo.GEO2D as a direction argument. Not a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2:27 in vid, OSM code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> use osm\n",
    "switched to db osm\n",
    "> db.nodes.find( {\"loc\":{\"$near\":[41.94,-87.65]}, \"tg\":{\"$exists\":1}).pretty()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Most Common City Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Use an aggregation query to answer the following question. \n",
    "\n",
    "What is the most common city name in our cities collection?\n",
    "\n",
    "Your first attempt probably identified None as the most frequently occurring\n",
    "city name. What that actually means is that there are a number of cities\n",
    "without a name field at all. It's strange that such documents would exist in\n",
    "this collection and, depending on your situation, might actually warrant\n",
    "further cleaning. \n",
    "\n",
    "To solve this problem the right way, we should really ignore cities that don't\n",
    "have a name specified. As a hint ask yourself what pipeline operator allows us\n",
    "to simply filter input? How do we test for the existence of a field?\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns\n",
    "an aggregation pipeline that can be passed to the MongoDB aggregate function.\n",
    "As in our examples in this lesson, the aggregation pipeline should be a list of\n",
    "one or more dictionary objects. Please review the lesson examples if you are\n",
    "unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you\n",
    "want to run this code locally on your machine, you have to install MongoDB, \n",
    "download and insert the dataset. For instructions related to MongoDB setup and\n",
    "datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a different version of the\n",
    "cities collection provided in the course materials. If you attempt some of the\n",
    "same queries that we look at in the problem set, your results may be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    \"\"\"\n",
    "    What is the most common city name in our cities collection?\n",
    "    \"\"\"\n",
    "    pipeline = [ \n",
    "        # Search only cities that have names\n",
    "        {\"$match\": {\"$name\": {\"$exists\": 1}}},\n",
    "        # Group cities with the same name and count number in each group\n",
    "        {\"$group\": {\"_id\": \"$name\",\n",
    "                    \"count\": {\"$sum\": 1}}},\n",
    "        # Sort groups by descending order\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        # Top entry\n",
    "        {\"$limit\": 1}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # The following statements will be used to test your code by the grader.\n",
    "    # Any modifications to the code past this point will not be reflected by\n",
    "    # the Test Run.\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert len(result) == 1\n",
    "    assert result[0] == {'_id': 'Shahpur', 'count': 6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistakes:\n",
    "\n",
    "* \"\\$fieldname\" instead of \"\\$fieldname.text\"\n",
    "* Extra curly or square brackets\n",
    "* Forgot quotes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
